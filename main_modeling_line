using PDDL, PlanningDomains
using SymbolicPlanners
import SymbolicPlanners: precompute!, is_precomputed, compute
using CSV

# Load domain and problem
domain_path = "/Users/joekwon/Desktop/line_modeling"
state_history_path = "/Users/joekwon/Desktop/line_modeling/state_histories"
data_save_path = "/Users/joekwon/Desktop/line_modeling/maps/data/"
multi_agent_lines = load_domain(joinpath(domain_path, "domain.pddl"))
problem_path = "/Users/joekwon/Desktop/line_modeling/maps"
#mal_problem = load_problem(joinpath(problem_path, "new_maybe_3.pddl"))
domain = multi_agent_lines

all_problems = ["no_line_1_test.pddl", "no_line_2_test.pddl", "no_line_3_test.pddl", "yes_line_7_test.pddl", "yes_line_8_test.pddl", "yes_line_9_test.pddl", "yes_line_10_test.pddl", "7esque_test.pddl", "9esque_test.pddl", "10esque_test.pddl", "maybe_4.pddl", "maybe_5.pddl", "maybe_6.pddl", "new_maybe_1.pddl", "new_maybe_2.pddl", "new_maybe_3.pddl", "new_maybe_4.pddl", "new_maybe_5.pddl", "new_maybe_6.pddl", "no_line_A.pddl", "no_line_B.pddl", "no_line_C.pddl", "no_line_D.pddl", "yes_line_A.pddl", "yes_line_B.pddl", "yes_line_C.pddl", "yes_line_D.pddl", "yes_line_E.pddl", "yes_line_F.pddl"]

# define function to modify state to be a single agent state (turning all other agents into walls)
function modify_state(state::State, k::Integer)
    # define objects and their types
    objtypes = PDDL.get_objtypes(state)
    # store the coordinates of all the agents that get deleted, to place walls there later
    agent_locs = Array{Tuple{Int,Int}}(undef, N-1)
    agent_index = 1
    # iterate over all agent objects. Unless it is the current agent, turn it into a wall
    for n in 1:N
        if n == k
            continue
        end
        agent = Const(Symbol("agent$n"))
        #save the coordinates of the agent, in order to place a wall there
        agent_locs[agent_index] = ((state[Compound(:xloc, Term[agent])], state[Compound(:yloc, Term[agent])]))
        agent_index += 1
        #then, delete the agent completely
        delete!(objtypes, agent)
    end
    # get the old wall matrix
    walls = copy(state[pddl"(walls)"])
    # it looks like this:
    # Bool[0 0 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0 1 1; 0 0 0 0 0 0 0 0 0 0 0 0 1 0; 0 0 0 0 0 0 0 0 0 0 1 1 1 0; 0 0 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 1 1 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0 1 0; 0 0 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    # modify wall matrix to include walls where agents were
    for loc in agent_locs
        walls[loc[2], loc[1]] = true
    end

    fluents = Dict{Term, Any}()
    # Loop over non-agents and assign their locations
    for (obj, objtype) in objtypes
        if objtype == :agent
            continue
        end
        fluents[Compound(:xloc, Term[obj])] = state[Compound(:xloc, Term[obj])]
        fluents[Compound(:yloc, Term[obj])] = state[Compound(:yloc, Term[obj])]
    end

    agent = Const(Symbol("agent$k"))

    # Assign walls fluent
    fluents[pddl"(walls)"] = walls
    # Assign position of remaining agent
    fluents[Compound(:xloc, Term[agent])] = state[Compound(:xloc, Term[agent])]
    fluents[Compound(:yloc, Term[agent])] = state[Compound(:yloc, Term[agent])]

    # new state is missing this: Set(Term[not(has-water(agent2)), not(has-filled(agent1)), not(has-water(agent1)), not(has-filled(agent2))])
    #fluents is a dictionary of assignments to fluents (will have true/false values, array vals, ..)
    #bools default false
    fluents[Compound(Symbol("has-filled"), Term[agent])] = state[Compound(Symbol("has-filled"), Term[agent])]
    fluents[Compound(Symbol("has-water1"), Term[agent])] = state[Compound(Symbol("has-water1"), Term[agent])]
    fluents[Compound(Symbol("has-water2"), Term[agent])] = state[Compound(Symbol("has-water2"), Term[agent])]
    fluents[Compound(Symbol("has-water3"), Term[agent])] = state[Compound(Symbol("has-water3"), Term[agent])]

    new_state = initstate(domain, objtypes, fluents)
    #print(new_state)
    return new_state
end

inner_heuristic = WellTankHeuristic() # Optimistic heuristic
planner = AStarPlanner(inner_heuristic) # Planner that uses optimistic heuristic
heuristic = PlannerHeuristic(planner) # Some exact heuristic
memoized_h = memoized(heuristic) # Memoized exact heuristic

N=8 

#define multi agent domain
struct MultiAgentDomain{D <: Domain} <: Domain
    domain::D
end #TODO: modify transition dynamic. allow multiple actions and handle collisions. was used in multi rtdp

#transition to roll out action and check for collisions
# function PDDL.transition(domain::MultiAgentDomain, state::State, actions)
#     agent_locs = []
#     #basically, right now we iterate through the list of agents in the same order each time setup
#     for (i, act) in enumerate(actions)
# 	    agent = Const(Symbol("agent$i"))
#         next_state = transition(domain.domain, state, act)
#         next_agent_loc = (next_state[Compound(:xloc, Term[agent])], next_state[Compound(:yloc, Term[agent])])

#         #if an agent tries to move into a square that a preceding agent has moved into, then this agent stays still
# 	    if next_agent_loc in agent_locs
# 	        agent_loc = (state[Compound(:xloc, Term[agent])], state[Compound(:yloc, Term[agent])]) 
#             push!(agent_locs, agent_loc)
#         #no collision, so move the agent into the new location
#         else
#             state = next_state
#             push!(agent_locs, next_agent_loc)
#         end
#     end
#     return state
# end

multi_domain = MultiAgentDomain(domain)


for problem in all_problems
    mal_problem = load_problem(joinpath(problem_path, problem))
    # Register array theory for gridworld domains
    PDDL.Arrays.@register()

    mal_state = initstate(multi_agent_lines, mal_problem)
    mal_spec = Specification(mal_problem)

    state = mal_state
    domain = multi_agent_lines

    T=1000 #500 time steps
    boltzmann_policy_parameters = [0.0001]

    data = Dict()
    for parameter in boltzmann_policy_parameters
        data[parameter] = Dict{Int64, Dict{Int64, Int64}}()
    end

    #iterating over all the boltzmann_policy_parameter values
    for noise in boltzmann_policy_parameters
        #iterate loop on each boltzmann_policy_parameter value 5 times each
        for iteration in 1:5
            #print("noise: ", noise, " iteration: ", iteration, "\n")

            state = initstate(domain, mal_problem)
            state_history = [state]

            boltzmann_policies = Array{BoltzmannPolicy}(undef, N)
            for n in 1:N
                agent = Const(Symbol("agent$n"))
                mal_spec = MinStepsGoal(Term[Compound(Symbol("has-filled"), Term[agent])])
                inner_policy = FunctionalVPolicy(memoized_h, domain, mal_spec) # Policy that evaluates every state
                boltzmann_policy = BoltzmannPolicy(inner_policy, noise) # Boltzmann agent
                boltzmann_policies[n] = boltzmann_policy
            end

            #create a dictionary where the key is the agent number and the value is the time step that the agent has-filled
            agent_filled = Dict{Int64, Int64}(1 => 0, 2 => 0, 3 => 0, 4 => 0, 5 => 0, 6 => 0, 7 => 0, 8 => 0)

            #for saving state_history
            state_history_name = string("state_history", "_", problem, "_", noise, "_iteration", iteration, ".txt")
            file = open(joinpath(state_history_path, state_history_name), "w")
            write(file, string(state))
            close(file)

            for t in 1:T
                #actions = Term[]
                interim_state = state
                for n in 1:N
                    agent = Const(Symbol("agent$n"))
                    modified_state = modify_state(interim_state, n) # Change other agents into walls
                    act = SymbolicPlanners.get_action(boltzmann_policies[n], modified_state)
                    interim_state = transition(domain, interim_state, act)
                end
            
                state = interim_state

                #save object state_history as text to a new file in current directory
                file = open(joinpath(state_history_path, state_history_name), "a")
                write(file, string(state))
                close(file)

                push!(state_history, state)
        
                # if any agent has filled the tank, then record the time step into the agent_filled dictionary
                for n in 1:N
                    agent = Const(Symbol("agent$n"))
                    if state[Compound(Symbol("has-filled"), Term[agent])]
                        #only change the value if it is equal to 0, otherwise constant overwriting.
                        if agent_filled[n] == 0
                            agent_filled[n] = t
                        end 
                    end
                end
        
                # if state didn't change, then set all agent_filled values to -1
                if state == state_history[t] && state == state_history[t-1] && state == state_history[t-2] 
                    for n in 1:N
                        agent_filled[n] = -1
                    end
                    break
                end
        
                # if all the goals have been met, terminate the loop
                if all([state[Compound(Symbol("has-filled"), Term[Const(Symbol("agent$n"))])] for n in 1:N])
                    break
                end
            end

            #add the agent_filled dictionary to the data dictionary
            data[noise][iteration] = agent_filled
            #print(data[noise][iteration], "\n")
        end
        csv_name = string(data_save_path, problem, "_", noise, ".csv")
        CSV.write(csv_name, data, append=true)
    end
end