using PDDL, PlanningDomains
using SymbolicPlanners
using DataFrames
import SymbolicPlanners: precompute!, is_precomputed, compute
using CSV

const domain_path = "/Users/joekwon/Desktop/line_modeling"
const state_history_path = "/Users/joekwon/Desktop/line_modeling/all_state_histories/recursive"
const data_save_path = "/Users/joekwon/Desktop/line_modeling/all_data/recursive/"
const problem_path = "/Users/joekwon/Desktop/line_modeling/maps"

#variables
const all_maps = ["no_line_1_test.pddl"]
const T = 100
const N = 8
const num_iterations = 1
boltzmann_policy_parameters = [0.0]
PDDL.Arrays.@register() 

#heuristic setup

#level 1 agent should assume level 0 is deterministic/optimal, but act with noise. A* search expects deterministic
function heuristic_setup()
    # Create a well tank heuristic
    inner_heuristic = WellTankHeuristic() # Optimistic heuristic
    planner = AStarPlanner(inner_heuristic) # Planner that uses optimistic heuristic
    heuristic = PlannerHeuristic(planner) # Some exact heuristic
    memoized_h = memoized(heuristic) # Memoized exact heuristic
    return memoized_h
end

#simulated domain setup
struct SimulatedDomain{D <: Domain, P} <: Domain
    domain::D
    agent::Term
    policies::Vector{P}
end

PDDL.satisfy(domain::SimulatedDomain, state::State, term::Term) =
    PDDL.satisfy(domain.domain, state, term)
PDDL.satisfy(domain::SimulatedDomain, state::State, terms::AbstractVector{<:Term}) =
    PDDL.satisfy(domain.domain, state, terms)
PDDL.satisfiers(domain::SimulatedDomain, state::State, terms::AbstractVector{<:Term}) =
    PDDL.satisfiers(domain.domain, state, terms)
PDDL.evaluate(domain::SimulatedDomain, state::State, term::Term) = 
    PDDL.evaluate(domain.domain, state, term)
PDDL.execute(domain::SimulatedDomain, state::State, action::Action, args; options...) = 
    PDDL.execute(domain.domain, state, action, args; options...)   
PDDL.execute(domain::SimulatedDomain, state::State, action::Term; options...) =
    PDDL.execute(domain.domain, state, action; options...)


PDDL.get_actions(domain::SimulatedDomain) = PDDL.get_actions(domain.domain)
PDDL.get_action(domain::SimulatedDomain, name::Symbol) = PDDL.get_action(domain.domain, name)
PDDL.get_axioms(domain::SimulatedDomain) = PDDL.get_axioms(domain.domain)
PDDL.get_axiom(domain::SimulatedDomain, name::Symbol) = PDDL.get_axiom(domain.domain, name)
PDDL.get_fluents(domain::SimulatedDomain) = PDDL.get_fluents(domain.domain)
PDDL.get_fluent(domain::SimulatedDomain, name::Symbol) = PDDL.get_fluent(domain.domain, Symbol)
PDDL.get_predicates(domain::SimulatedDomain) = PDDL.get_predicates(domain.domain)
PDDL.get_predicate(domain::SimulatedDomain, name::Symbol) = PDDL.get_predicate(domain.domain, name)
PDDL.get_name(domain::SimulatedDomain) = PDDL.get_name(domain.domain)
PDDL.get_source(domain::SimulatedDomain) = PDDL.get_source(domain.domain)
PDDL.get_requirements(domain::SimulatedDomain) = PDDL.get_requirements(domain.domain)
PDDL.get_typetree(domain::SimulatedDomain) = PDDL.get_typetree(domain.domain)
PDDL.get_datatypes(domain::SimulatedDomain) = PDDL.get_datatypes(domain.domain)
PDDL.get_constants(domain::SimulatedDomain) = PDDL.get_constants(domain.domain)
PDDL.get_constypes(domain::SimulatedDomain) = PDDL.get_constypes(domain.domain)
PDDL.get_constype(domain::SimulatedDomain, obj) = PDDL.get_constypes(domain.domain, obj)
PDDL.get_types(domain::SimulatedDomain) = PDDL.get_types(domain.domain)
PDDL.get_subtypes(domain::SimulatedDomain, type::Symbol) = PDDL.get_subtypes(domain.domain, type)
PDDL.get_datatype(domain::SimulatedDomain, datatype::Symbol) = PDDL.get_datatype(domain.domain, datatype)
PDDL.get_functions(domain::SimulatedDomain) = PDDL.get_functions(domain.domain)
PDDL.get_function(domain::SimulatedDomain, name::Symbol) = PDDL.get_function(domain.domain, name)
PDDL.get_funcdefs(domain::SimulatedDomain) = PDDL.get_funcdefs(domain.domain)
PDDL.get_funcdef(domain::SimulatedDomain, name::Symbol) = PDDL.get_funcdef(domain.domain, name)


# Returns a vector of level 0 policies." 
function create_level0_policies(N, domain, noise=0.0)
    # Construct heuristic / value function that does planning internally
    heuristic = heuristic_setup()
    # Construct a policy for each agent
    policies = map(1:N) do n
        agent = Const(Symbol("agent$n"))
        spec = MinStepsGoal(Term[Compound(Symbol("has-filled"), Term[agent])])
        policy = FunctionalVPolicy(heuristic, domain, spec)
        # Wrap in a Boltzmann policy if noise is not zero
        if noise != 0.0
            policy = BoltzmannPolicy(policy, noise) # Boltzmann agent
        end
        return policy
    end
    return policies
end

# Returns a vector of level 1 policies, " 
function create_level1_policies(N, domain, level0_policies, noise=0.0)
    # Construct heuristic / value function that does planning internally
    heuristic = heuristic_setup()
    # Construct a level-1 policy for each agent
    policies = map(1:N) do n
        agent = Const(Symbol("agent$n"))
        spec = MinStepsGoal(Term[Compound(Symbol("has-filled"), Term[agent])])
        # Construct simulated domain for the level 1 agent, using the level 0 policies
        sim_domain = SimulatedDomain(domain, agent, level0_policies)
        # Construct a policy that plans using the simulated domain
        policy = FunctionalVPolicy(heuristic, sim_domain, spec)
        # Wrap in a Boltzmann policy if noise is not zero
        if noise != 0.0
            policy = BoltzmannPolicy(policy, noise) # Boltzmann agent
        end
        return policy
    end
    return policies
end

function modify_state(state::State, domain::SimulatedDomain)
    # Get the object types of the state
    objtypes = PDDL.get_objtypes(state)

    agent_locs = Array{Tuple{Int,Int}}(undef, N-1)
    agent_index = 1
    # iterate over all agent objects. Unless it is the current agent, turn it into a wall
    for n in 1:N
        if domain.agent == Const(Symbol("agent$n"))
            continue
        end
        other_agent = Const(Symbol("agent$n"))
        #save the coordinates of the agent, in order to place a wall there
        agent_locs[agent_index] = ((state[Compound(:xloc, Term[other_agent])], state[Compound(:yloc, Term[other_agent])]))
        agent_index += 1
        #then, delete the agent completely
        delete!(objtypes, other_agent)
    end

    # Get the old wall matrix and modify it to include walls where agents were
    walls = copy(state[pddl"(walls)"])
    for loc in agent_locs
        walls[loc[2], loc[1]] = true
    end

    # Create a dictionary of fluents and loop over non-agents to assign their locations
    fluents = Dict{Term, Any}()
    for (obj, objtype) in objtypes
        if objtype != :agent
            fluents[Compound(:xloc, Term[obj])] = state[Compound(:xloc, Term[obj])]
            fluents[Compound(:yloc, Term[obj])] = state[Compound(:yloc, Term[obj])]
        end
    end

    # Assign the wall fluent and position of the remaining agent
    agent = domain.agent
    fluents[pddl"(walls)"] = walls
    fluents[Compound(:xloc, Term[agent])] = state[Compound(:xloc, Term[agent])]
    fluents[Compound(:yloc, Term[agent])] = state[Compound(:yloc, Term[agent])]

    # Assign values to boolean fluents
    boolean_fluents = [Symbol("has-filled"), Symbol("has-water1"), Symbol("has-water2"), Symbol("has-water3"), Symbol("has-completed")]
    for boolean_fluent in boolean_fluents
        fluents[Compound(boolean_fluent, Term[agent])] = state[Compound(boolean_fluent, Term[agent])]
    end

    return initstate(domain.domain, objtypes, fluents)
end

function modify_state_2(state::State, domain::SimulatedDomain)
    # Get the object types of the state
    objtypes = PDDL.get_objtypes(state)

    # iterate over all agent objects. Unless it is the current agent, delete the agent
    for n in 1:N
        if domain.agent == Const(Symbol("agent$n"))
            continue
        end
        other_agent = Const(Symbol("agent$n"))
        delete!(objtypes, other_agent)
    end

    # Get the old wall matrix 
    walls = copy(state[pddl"(walls)"])

    # Create a dictionary of fluents and loop over non-agents to assign their locations
    fluents = Dict{Term, Any}()
    for (obj, objtype) in objtypes
        if objtype != :agent
            fluents[Compound(:xloc, Term[obj])] = state[Compound(:xloc, Term[obj])]
            fluents[Compound(:yloc, Term[obj])] = state[Compound(:yloc, Term[obj])]
        end
    end

    # Assign the wall fluent and position of the remaining agent
    agent = domain.agent
    fluents[pddl"(walls)"] = walls
    fluents[Compound(:xloc, Term[agent])] = state[Compound(:xloc, Term[agent])]
    fluents[Compound(:yloc, Term[agent])] = state[Compound(:yloc, Term[agent])]

    # Assign values to boolean fluents
    boolean_fluents = [Symbol("has-filled"), Symbol("has-water1"), Symbol("has-water2"), Symbol("has-water3"), Symbol("has-completed")]
    for boolean_fluent in boolean_fluents
        fluents[Compound(boolean_fluent, Term[agent])] = state[Compound(boolean_fluent, Term[agent])]
    end

    return initstate(domain.domain, objtypes, fluents)
end

# Define function to write state history to file
function write_state_history_to_file(file_path::String, state::State, edit_type::String)
    if edit_type == "init"
        file = open(file_path, "w")
    else
        file = open(file_path, "a")
    end
    write(file, string(state))
    close(file)
end

# initialize setup for each problem
function initialize(domain_path, problem_path, map)
    problem = load_problem(joinpath(problem_path, map))
    domain = load_domain(joinpath(domain_path, "domain.pddl"))
    state, mal_spec = initialize_state(domain, problem)
    return state, domain, mal_spec
end

# Define function to initialize state
function initialize_state(domain, problem)
    state = initstate(domain, problem)
    mal_spec = Specification(problem)
    return state, mal_spec
end

function PDDL.available(domain::SimulatedDomain, state::State)
    # Modify state to turn all other agents to stone
    modified_state = modify_state_2(state, domain)
    # Return actions that are available for acting agent in modified state
    return available(domain.domain, modified_state)
end

# function PDDL.available(domain::SimulatedDomain, state::State)
#     # Simulate one step of each of the other agent's policies
#     for agent_index in 1:N
#         if domain.agent == Const(Symbol("agent$agent_index"))
#             continue
#         end
#         policy = domain.policies[agent_index]
#         modified_state = modify_state(state, domain)
#         act = SymbolicPlanners.get_action(policy, modified_state)
#         state = transition(domain.domain, state, act)
#     end
#     # Modify state to turn all other agents to stone
#     modified_state = modify_state(state, domain)
#     # Return actions that are available for acting agent in modified state
#     return available(domain.domain, modified_state)
# end

function try_transition(domain, state, act)
    if !available(domain, state, act)
        return state
    else
        return transition(domain, state, act; check=false)
    end
end

function PDDL.transition(domain::SimulatedDomain, state::State, action::Term)
    # Simulate one step of each of the other agent's policies
    for agent_index in 1:N
        if domain.agent == Const(Symbol("agent$agent_index"))
            continue
        end
        policy = domain.policies[agent_index]
        modified_state = modify_state(state, domain)
        act = SymbolicPlanners.get_action(policy, modified_state)
        state = transition(domain.domain, state, act)
    end
    # Simulate own action
    state = try_transition(domain.domain, state, action)
    return state
end

#function to run simulations
function run_simulations(all_maps, boltzmann_policy_parameters, num_iterations, T,
    domain_path, problem_path, state_history_path, data_save_path)
    for map in all_maps
        
        data = Dict{Float64, Dict{Int64, Dict{Int64, Int64}}}()
        for parameter in boltzmann_policy_parameters
            data[parameter] = Dict{Int64, Dict{Int64, Int64}}()
        end

        for noise in boltzmann_policy_parameters
            for iteration in 1:num_iterations
                state, domain, mal_spec = initialize(domain_path, problem_path, map)
                state_history = [state]

                level0_policies = create_level0_policies(N, domain, noise)
                level1_policies = create_level1_policies(N, domain, level0_policies, noise)

                agent_filled = Dict{Int64, Int64}(1 => 0, 2 => 0, 3 => 0, 4 => 0, 5 => 0, 6 => 0, 7 => 0, 8 => 0)

                state_history_name = "state_history_$(map)_$(noise)_iteration$(iteration).txt"
                write_state_history_to_file(joinpath(state_history_path, state_history_name), state, "init")

                for t in 1:T
                    #get the actions for each agent, and transition the state
                    for n in 1:N
                        agent = Const(Symbol("agent$n"))
                        act = SymbolicPlanners.get_action(level1_policies[n], state)
                        state = try_transition(domain, state, act)
                    end

                    write_state_history_to_file(joinpath(state_history_path, state_history_name), state, "append")
                    push!(state_history, state)
            
                    for n in 1:N
                        agent = Const(Symbol("agent$n"))
                        #only change the value if it is equal to 0, otherwise constant overwriting.
                        if state[Compound(Symbol("has-filled"), Term[agent])] && agent_filled[n] == 0
                            agent_filled[n] = t
                        end
                    end
            
                    if state == state_history[t] && state == state_history[t-1] && state == state_history[t-2] 
                        for n in 1:N
                            agent_filled[n] = -1
                        end
                        break
                    end
            
                    if all([state[Compound(Symbol("has-filled"), Term[Const(Symbol("agent$n"))])] for n in 1:N])
                        break
                    end
                end

                data[noise][iteration] = agent_filled
            end
            csv_name = "$(data_save_path)$(map)_$(noise).csv"
            CSV.write(csv_name, data, append=true)
        end
    end
    print("Finished!")
end

run_simulations(all_maps, boltzmann_policy_parameters, num_iterations, T,
    domain_path, problem_path, state_history_path, data_save_path)

