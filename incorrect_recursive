using PDDL, PlanningDomains
using SymbolicPlanners
import SymbolicPlanners: precompute!, is_precomputed, compute
using CSV

const domain_path = "/Users/joekwon/Desktop/line_modeling"
const state_history_path = "/Users/joekwon/Desktop/line_modeling/all_state_histories/naive_recursive"
const data_save_path = "/Users/joekwon/Desktop/line_modeling/all_data/naive_recursive/"
const problem_path = "/Users/joekwon/Desktop/line_modeling/maps"

#variables
const all_maps = ["no_line_1_test.pddl", "no_line_2_test.pddl", "no_line_3_test.pddl", "yes_line_7_test.pddl", "yes_line_8_test.pddl", "yes_line_9_test.pddl", "yes_line_10_test.pddl", "7esque_test.pddl", "9esque_test.pddl", "10esque_test.pddl", "maybe_4.pddl", "maybe_5.pddl", "maybe_6.pddl", "new_maybe_1.pddl", "new_maybe_2.pddl", "new_maybe_3.pddl", "new_maybe_4.pddl", "new_maybe_5.pddl", "new_maybe_6.pddl", "no_line_A.pddl", "no_line_B.pddl", "no_line_C.pddl", "no_line_D.pddl", "yes_line_A.pddl", "yes_line_B.pddl", "yes_line_C.pddl", "yes_line_D.pddl", "yes_line_E.pddl", "yes_line_F.pddl"]
const T = 800
const N = 8 
const num_iterations = 5
boltzmann_policy_parameters = [0.0001]
PDDL.Arrays.@register() 

#heuristic setup

#level 1 agent should assume level 0 is deterministic/optimal, but act with noise. A* search expects deterministic
function heuristic_setup()
    # Create a well tank heuristic
    inner_heuristic = WellTankHeuristic() # Optimistic heuristic
    planner = AStarPlanner(inner_heuristic) # Planner that uses optimistic heuristic
    heuristic = PlannerHeuristic(planner) # Some exact heuristic
    memoized_h = memoized(heuristic) # Memoized exact heuristic
    return memoized_h
end

memoized_h = heuristic_setup()

#domain setup
struct MultiAgentDomain{D <: Domain} <: Domain
    domain::D
end 

# Define function to create boltzmann policies
function create_boltzmann_policies(N, domain, mal_spec, noise)
    policies = Array{BoltzmannPolicy}(undef, N)
    for n in 1:N
        agent = Const(Symbol("agent$n"))
        mal_spec = MinStepsGoal(Term[Compound(Symbol("has-filled"), Term[agent])])
        inner_policy = FunctionalVPolicy(memoized_h, domain, mal_spec) # Policy that evaluates every state
        boltzmann_policy = BoltzmannPolicy(inner_policy, noise) # Boltzmann agent
        policies[n] = boltzmann_policy
    end
    return policies
end

function modify_state(state::State, k::Integer, domain)
    # Get the object types of the state
    objtypes = PDDL.get_objtypes(state)

    agent_locs = Array{Tuple{Int,Int}}(undef, N-1)
    agent_index = 1
    # iterate over all agent objects. Unless it is the current agent, turn it into a wall
    for n in 1:N
        if n == k
            continue
        end
        agent = Const(Symbol("agent$n"))
        #save the coordinates of the agent, in order to place a wall there
        agent_locs[agent_index] = ((state[Compound(:xloc, Term[agent])], state[Compound(:yloc, Term[agent])]))
        agent_index += 1
        #then, delete the agent completely
        delete!(objtypes, agent)
    end

    # Get the old wall matrix and modify it to include walls where agents were
    walls = copy(state[pddl"(walls)"])
    for loc in agent_locs
        walls[loc[2], loc[1]] = true
    end

    # Create a dictionary of fluents and loop over non-agents to assign their locations
    fluents = Dict{Term, Any}()
    for (obj, objtype) in objtypes
        if objtype != :agent
            fluents[Compound(:xloc, Term[obj])] = state[Compound(:xloc, Term[obj])]
            fluents[Compound(:yloc, Term[obj])] = state[Compound(:yloc, Term[obj])]
        end
    end

    # Assign the wall fluent and position of the remaining agent
    agent = Const(Symbol("agent$k"))
    fluents[pddl"(walls)"] = walls
    fluents[Compound(:xloc, Term[agent])] = state[Compound(:xloc, Term[agent])]
    fluents[Compound(:yloc, Term[agent])] = state[Compound(:yloc, Term[agent])]

    # Assign values to boolean fluents
    boolean_fluents = [Symbol("has-filled"), Symbol("has-water1"), Symbol("has-water2"), Symbol("has-water3")]
    for boolean_fluent in boolean_fluents
        fluents[Compound(boolean_fluent, Term[agent])] = state[Compound(boolean_fluent, Term[agent])]
    end

    return initstate(domain, objtypes, fluents)
end

function PDDL.transition(domain::MultiAgentDomain, state::State, actions::Vector{Term})
    agent_locs = []
    
    for (i, act) in enumerate(actions)
        agent = Const(Symbol("agent$i"))
        next_state = transition(domain.domain, state, act)
        next_agent_loc = (next_state[Compound(:xloc, Term[agent])], next_state[Compound(:yloc, Term[agent])])
        
        if next_agent_loc in agent_locs
            agent_loc = (state[Compound(:xloc, Term[agent])], state[Compound(:yloc, Term[agent])]) 
            push!(agent_locs, agent_loc)
        else
            state = next_state
            push!(agent_locs, next_agent_loc)
        end
    end
    
    return state
end

function transition_other_agents(domain::MultiAgentDomain, state::State, actions::Vector{Term}, current_agent::Int64)
    agent_locs = []
    
    for (i, act) in enumerate(actions)
        if i == current_agent
            continue
        end
        agent = Const(Symbol("agent$i"))
        #TODO verify what transition function is being used.
        next_state = transition(domain.domain, state, act)
        next_agent_loc = (next_state[Compound(:xloc, Term[agent])], next_state[Compound(:yloc, Term[agent])])
        
        if next_agent_loc in agent_locs
            agent_loc = (state[Compound(:xloc, Term[agent])], state[Compound(:yloc, Term[agent])]) 
            push!(agent_locs, agent_loc)
        else
            state = next_state
            push!(agent_locs, next_agent_loc)
        end
    end
    
    return state
end

function get_main_agent_action(boltzmann_policies::Vector{BoltzmannPolicy}, state::State, N::Int64, domain, current_agent::Int64)
    agent = Const(Symbol("agent$current_agent"))
    modified_state = modify_state(state, current_agent, domain)
    act = SymbolicPlanners.get_action(boltzmann_policies[current_agent], modified_state)
    return act
end

function get_other_agent_actions(boltzmann_policies::Vector{BoltzmannPolicy}, state::State, N::Int64, domain, current_agent::Int64)
    actions = Term[]
    for n in 1:N
        if n == current_agent
            push!(actions, pddl"wait")
            continue
        end
        agent = Const(Symbol("agent$n"))
        modified_state = modify_state(state, n, domain)
        act = SymbolicPlanners.get_action(boltzmann_policies[n], modified_state)
        push!(actions, act)
    end
    return actions
end

# Define function to write state history to file
function write_state_history_to_file(file_path::String, state::State, edit_type::String)
    if edit_type == "init"
        file = open(file_path, "w")
    else
        file = open(file_path, "a")
    end
    write(file, string(state))
    close(file)
end

# initialize setup for each problem
function initialize(domain_path, problem_path, map)
    problem = load_problem(joinpath(problem_path, map))
    domain = load_domain(joinpath(domain_path, "domain.pddl"))
    state, multi_domain, mal_spec = initialize_state(domain, problem)
    return state, domain, multi_domain, mal_spec
end

# Define function to initialize state
function initialize_state(domain, problem)
    state = initstate(domain, problem)
    multi_domain = MultiAgentDomain(domain)
    mal_spec = Specification(problem)
    return state, multi_domain, mal_spec
end

#function to run simulations
function run_simulations(all_maps, boltzmann_policy_parameters, num_iterations, T,
    domain_path, problem_path, state_history_path, data_save_path)
    for map in all_maps
        
        data = Dict{Float64, Dict{Int64, Dict{Int64, Int64}}}()
        for parameter in boltzmann_policy_parameters
            data[parameter] = Dict{Int64, Dict{Int64, Int64}}()
        end

        for noise in boltzmann_policy_parameters
            for iteration in 1:num_iterations
                state, domain, multi_domain, mal_spec = initialize(domain_path, problem_path, map)
                state_history = [state]

                boltzmann_policies = create_boltzmann_policies(N, domain, mal_spec, noise)

                agent_filled = Dict{Int64, Int64}(1 => 0, 2 => 0, 3 => 0, 4 => 0, 5 => 0, 6 => 0, 7 => 0, 8 => 0)

                state_history_name = "state_history_$(map)_$(noise)_iteration$(iteration).txt"
                write_state_history_to_file(joinpath(state_history_path, state_history_name), state, "init")

                for t in 1:T
                    actions = Term[]
                    for current_agent in 1:N
                        simulated_actions = get_other_agent_actions(boltzmann_policies, state, N, domain, current_agent)
                        simulated_state = transition_other_agents(multi_domain, state, simulated_actions, current_agent)
                        act = get_main_agent_action(boltzmann_policies, simulated_state, N, domain, current_agent)
                        push!(actions, act)
                    end
                    state = PDDL.transition(multi_domain, state, actions)

                    write_state_history_to_file(joinpath(state_history_path, state_history_name), state, "append")
                    push!(state_history, state)
            
                    for n in 1:N
                        agent = Const(Symbol("agent$n"))
                        #only change the value if it is equal to 0, otherwise constant overwriting.
                        if state[Compound(Symbol("has-filled"), Term[agent])] && agent_filled[n] == 0
                            agent_filled[n] = t
                        end
                    end
            
                    if state == state_history[t] && state == state_history[t-1] && state == state_history[t-2] 
                        for n in 1:N
                            agent_filled[n] = -1
                        end
                        break
                    end
            
                    if all([state[Compound(Symbol("has-filled"), Term[Const(Symbol("agent$n"))])] for n in 1:N])
                        break
                    end
                end

                data[noise][iteration] = agent_filled
            end
            csv_name = "$(data_save_path)$(map)_$(noise).csv"
            CSV.write(csv_name, data, append=true)
        end
    end
    print("Finished!")
end

run_simulations(all_maps, boltzmann_policy_parameters, num_iterations, T,
    domain_path, problem_path, state_history_path, data_save_path)

#todo: add in what kind of modeling we want into the run_simulations function


