using PDDL, PlanningDomains
using SymbolicPlanners
import SymbolicPlanners: precompute!, is_precomputed, compute
include("ascii.jl")


# Load domain and problem
domain_path = "/Users/joekwon/Desktop/line_modeling"
multi_agent_lines = load_domain(joinpath(domain_path, "domain.pddl"))
problem_path = "/Users/joekwon/Desktop/line_modeling/maps/"
mal_problem = load_problem(joinpath(problem_path, "no_line_1_test.pddl"))
#i can use readdir and regex matching syntax like 
# match(r"problem-\d.pddl", path)


# Register array theory for gridworld domains
PDDL.Arrays.@register()

mal_state = initstate(multi_agent_lines, mal_problem)
mal_spec = Specification(mal_problem)

state = mal_state
domain = multi_agent_lines
#problem = mal_problem
actions = available(domain, state)
#actions = available(multi_agent_lines, state)
#state = execute(domain, state, pddl"(up(agent1))")
#c_domain, c_state = compiled(domain, state)

#deprecated Oct 4
#planner = AStarPlanner(WellTankHeuristic())
#heuristic = WellTankHeuristic()
#/deprecated Oct 4

#print_ascii(state)


# define function to modify state to be a single agent state (turning all other agents into walls)
function modify_state(state::State, k::Integer)
    # define objects and their types
    objtypes = PDDL.get_objtypes(state)
    # store the coordinates of all the agents that get deleted, to place walls there later
    agent_locs = Array{Tuple{Int,Int}}(undef, N-1)
    agent_index = 1
    # iterate over all agent objects. Unless it is the current agent, turn it into a wall
    for n in 1:N
        if n == k
            continue
        end
        agent = Const(Symbol("agent$n"))
        #save the coordinates of the agent, in order to place a wall there
        agent_locs[agent_index] = ((state[Compound(:xloc, Term[agent])], state[Compound(:yloc, Term[agent])]))
        agent_index += 1
        #then, delete the agent completely
        delete!(objtypes, agent)
    end
    # get the old wall matrix
    walls = copy(state[pddl"(walls)"])
    # it looks like this:
    # Bool[0 0 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0 1 1; 0 0 0 0 0 0 0 0 0 0 0 0 1 0; 0 0 0 0 0 0 0 0 0 0 1 1 1 0; 0 0 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 1 1 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0 1 0; 0 0 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0 0 0; 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
    # modify wall matrix to include walls where agents were
    for loc in agent_locs
        walls[loc[2], loc[1]] = true
    end

    fluents = Dict{Term, Any}()
    # Loop over non-agents and assign their locations
    for (obj, objtype) in objtypes
        if objtype == :agent
            continue
        end
        fluents[Compound(:xloc, Term[obj])] = state[Compound(:xloc, Term[obj])]
        fluents[Compound(:yloc, Term[obj])] = state[Compound(:yloc, Term[obj])]
    end

    agent = Const(Symbol("agent$k"))

    # Assign walls fluent
    fluents[pddl"(walls)"] = walls
    # Assign position of remaining agent
    fluents[Compound(:xloc, Term[agent])] = state[Compound(:xloc, Term[agent])]
    fluents[Compound(:yloc, Term[agent])] = state[Compound(:yloc, Term[agent])]

    # new state is missing this: Set(Term[not(has-water(agent2)), not(has-filled(agent1)), not(has-water(agent1)), not(has-filled(agent2))])
    #fluents is a dictionary of assignments to fluents (will have true/false values, array vals, ..)
    #bools default false
    fluents[Compound(Symbol("has-filled"), Term[agent])] = state[Compound(Symbol("has-filled"), Term[agent])]
    fluents[Compound(Symbol("has-water1"), Term[agent])] = state[Compound(Symbol("has-water1"), Term[agent])]
    fluents[Compound(Symbol("has-water2"), Term[agent])] = state[Compound(Symbol("has-water2"), Term[agent])]
    fluents[Compound(Symbol("has-water3"), Term[agent])] = state[Compound(Symbol("has-water3"), Term[agent])]

    new_state = initstate(domain, objtypes, fluents)
    #print(new_state)
    return new_state
end

#modify_state(state, 1)
#print(state)

# TODO: Create a policy for each agent since goal spec is different for each agent,changing specification for each agent. 
inner_heuristic = WellTankHeuristic() # Optimistic heuristic
planner = AStarPlanner(inner_heuristic) # Planner that uses optimistic heuristic
heuristic = PlannerHeuristic(planner) # Some exact heuristic
memoized_h = memoized(heuristic) # Memoized exact heuristic

N=8 # number of agents

#define multi agent domain
struct MultiAgentDomain{D <: Domain} <: Domain
    domain::D
end #TODO: modify transition dynamic. allow multiple actions and handle collisions. was used in multi rtdp

#transition to roll out action and check for collisions
function PDDL.transition(domain::MultiAgentDomain, state::State, actions)
    agent_locs = []
    #basically, right now we iterate through the list of agents in the same order each time setup
    for (i, act) in enumerate(actions)
	    agent = Const(Symbol("agent$i"))
        next_state = transition(domain.domain, state, act)
        next_agent_loc = (next_state[Compound(:xloc, Term[agent])], next_state[Compound(:yloc, Term[agent])])

        #if an agent tries to move into a square that a preceding agent has moved into, then this agent stays still
	    if next_agent_loc in agent_locs
	        agent_loc = (state[Compound(:xloc, Term[agent])], state[Compound(:yloc, Term[agent])]) 
            push!(agent_locs, agent_loc)
        #no collision, so move the agent into the new location
        else
            state = next_state
            push!(agent_locs, next_agent_loc)
        end
    end
    return state
end

multi_domain = MultiAgentDomain(domain)
# Simulation loop
# state = initstate(domain, problem)

#modified_state = modify_state(state, 1)
#act = SymbolicPlanners.get_action(boltzmann_policies[1], modified_state)
#when defining WTHeuristic for the first time, make sure it's mutable
T=500 #500 time steps
boltzmann_policy_parameters = [0.00001]

##########
###############
####################
########################
#isolated code block for converting agent actions into videos
noise = 0.00001
action_history = Dict()

state = initstate(domain, mal_problem)
state_history = [state]
boltzmann_policies = Array{BoltzmannPolicy}(undef, N)
for n in 1:N
    agent = Const(Symbol("agent$n"))
    mal_spec = MinStepsGoal(Term[Compound(Symbol("has-filled"), Term[agent])])
    inner_policy = FunctionalVPolicy(memoized_h, domain, mal_spec) # Policy that evaluates every state
    boltzmann_policy = BoltzmannPolicy(inner_policy, noise) # Boltzmann agent
    boltzmann_policies[n] = boltzmann_policy
end

agent_filled = Dict{Int64, Int64}(1 => 0, 2 => 0, 3 => 0, 4 => 0, 5 => 0, 6 => 0, 7 => 0, 8 => 0)
file = open(joinpath(domain_path, "state_history.txt"), "w")
write(file, string(state))
close(file)

for t in 1:500
    actions = Term[]
    for n in 1:N
        agent = Const(Symbol("agent$n"))
        modified_state = modify_state(state, n) # Change other agents into walls
        act = SymbolicPlanners.get_action(boltzmann_policies[n], modified_state)
        push!(actions, act)
    end

    state = PDDL.transition(multi_domain, state, actions)

    #save object state_history as text to a new file in current directory
    file = open(joinpath(domain_path, "state_history.txt"), "a")
    write(file, string(state))
    close(file)

    push!(state_history, state)

    # if any agent has filled the tank, then record the time step into the agent_filled dictionary
    for n in 1:N
        agent = Const(Symbol("agent$n"))
        if state[Compound(Symbol("has-filled"), Term[agent])]
            #only change the value if it is equal to 0, otherwise constant overwriting.
            if agent_filled[n] == 0
                agent_filled[n] = t
            end 
        end
    end

    # if state didn't change, then set all agent_filled values to -1
    if state == state_history[t] && state == state_history[t-1] && state == state_history[t-2] 
        for n in 1:N
            agent_filled[n] = -1
        end
        break
    end

    # if all the goals have been met, terminate the loop
    if all([state[Compound(Symbol("has-filled"), Term[Const(Symbol("agent$n"))])] for n in 1:N])
        break
    end
end

print(state_history)
print(agent_filled)

#end of isolated code block
########################
###################
###############

#Initalize a dictionary of dictionaries where the keys are the boltzmann_policy_parameter values
#and the values are dictionaries where the key is the iteration number and the value is a dictionary where
#the key is the agent number and the value is the time step that the agent has-filled
data = Dict()
for parameter in boltzmann_policy_parameters
    data[parameter] = Dict{Int64, Dict{Int64, Int64}}()
end

#iterating over all the boltzmann_policy_parameter values
for noise in boltzmann_policy_parameters
    #iterate loop on each boltzmann_policy_parameter value 5 times each
    for iteration in 1:5
        print("noise: ", noise, " iteration: ", iteration, "\n")

        state = initstate(domain, mal_problem)
        state_history = [state]

        boltzmann_policies = Array{BoltzmannPolicy}(undef, N)
        for n in 1:N
            agent = Const(Symbol("agent$n"))
            mal_spec = MinStepsGoal(Term[Compound(Symbol("has-filled"), Term[agent])])
            inner_policy = FunctionalVPolicy(memoized_h, domain, mal_spec) # Policy that evaluates every state
            boltzmann_policy = BoltzmannPolicy(inner_policy, noise) # Boltzmann agent
            boltzmann_policies[n] = boltzmann_policy
        end

        #create a dictionary where the key is the agent number and the value is the time step that the agent has-filled
        agent_filled = Dict{Int64, Int64}(1 => 0, 2 => 0, 3 => 0, 4 => 0, 5 => 0, 6 => 0, 7 => 0, 8 => 0)

        for t in 1:T
            actions = Term[]
            for n in 1:N
                agent = Const(Symbol("agent$n"))
                modified_state = modify_state(state, n) # Change other agents into walls
                act = SymbolicPlanners.get_action(boltzmann_policies[n], modified_state)
                push!(actions, act)
            end
        
            state = PDDL.transition(multi_domain, state, actions)
            push!(state_history, state)
    
            # if any agent has filled the tank, then record the time step into the agent_filled dictionary
            for n in 1:N
                agent = Const(Symbol("agent$n"))
                if state[Compound(Symbol("has-filled"), Term[agent])]
                    #only change the value if it is equal to 0, otherwise constant overwriting.
                    if agent_filled[n] == 0
                        agent_filled[n] = t
                    end 
                end
            end
    
            # if state didn't change, then set all agent_filled values to -1
            if state == state_history[t] && state == state_history[t-1] && state == state_history[t-2] 
                for n in 1:N
                    agent_filled[n] = -1
                end
                break
            end
    
            # if all the goals have been met, terminate the loop
            if all([state[Compound(Symbol("has-filled"), Term[Const(Symbol("agent$n"))])] for n in 1:N])
                break
            end
        end

        #add the agent_filled dictionary to the data dictionary
        data[noise][iteration] = agent_filled
        print(data[noise][iteration], "\n")
    end
end

#end method 1 (total)
#end method 1 (total)

#end method 1 (total)
#end method 1 (total)
#end method 1 (total)
#end method 1 (total)
#end method 1 (total)
#end method 1 (total)
#end method 1 (total)



state = initstate(domain, mal_problem)
state_history = [state]

boltzmann_policies = Array{BoltzmannPolicy}(undef, N)
for n in 1:N
    agent = Const(Symbol("agent$n"))
    mal_spec = MinStepsGoal(Term[Compound(Symbol("has-filled"), Term[agent])])
    inner_policy = FunctionalVPolicy(memoized_h, domain, mal_spec) # Policy that evaluates every state
    boltzmann_policy = BoltzmannPolicy(inner_policy, 0.01) # Boltzmann agent
    boltzmann_policies[n] = boltzmann_policy
end

#create a dictionary where the key is the agent number and the value is the time step that the agent has-filled
agent_filled = Dict{Int64, Int64}(1 => 0, 2 => 0, 3 => 0, 4 => 0, 5 => 0, 6 => 0, 7 => 0, 8 => 0)

for t in 1:T
    actions = Term[]
    for n in 1:N
        agent = Const(Symbol("agent$n"))
        modified_state = modify_state(state, n) # Change other agents into walls
        act = SymbolicPlanners.get_action(boltzmann_policies[n], modified_state)
        push!(actions, act)
    end

    state = PDDL.transition(multi_domain, state, actions)
    push!(state_history, state)

    # if any agent has filled the tank, then record the time step into the agent_filled dictionary
    for n in 1:N
        agent = Const(Symbol("agent$n"))
        if state[Compound(Symbol("has-filled"), Term[agent])]
            #only change the value if it is equal to 0, otherwise constant overwriting.
            if agent_filled[n] == 0
                agent_filled[n] = t
            end 
        end
    end

    # visualizing with ascii
    print_ascii(state)
    print("time step: ")
    print(t)
    print("\n")

    # if state didn't change, then set all agent_filled values to -1
    if state == state_history[t] && state == state_history[t-1] && state == state_history[t-2] 
        for n in 1:N
            agent_filled[n] = -1
        end
        break
    end

    # if all the goals have been met, terminate the loop
    if all([state[Compound(Symbol("has-filled"), Term[Const(Symbol("agent$n"))])] for n in 1:N])
        break
    end
end
print(state)
print(agent_filled)

#end method 2 (5 iterations, one noise)


boltzmann_policies = Vector{Any}(undef, N)

#specification change here. mal_spec passed in should be different for each agent.
for n in 1:N
    agent = Const(Symbol("agent$n"))
    mal_spec = MinStepsGoal(Term[Compound(Symbol("has-filled"), Term[agent])])
    inner_policy = FunctionalVPolicy(memoized_h, domain, mal_spec) # Policy that evaluates every state
    boltzmann_policy = BoltzmannPolicy(inner_policy, 1.0) # Boltzmann agent
    boltzmann_policies[n] = boltzmann_policy
end


T=500 #500 time steps
state = initstate(domain, mal_problem)
state_history = [state]
for t in 1:T
    actions = Term[]
    for n in 1:N
        agent = Const(Symbol("agent$n"))
        modified_state = modify_state(state, n) # Change other agents into walls
        act = SymbolicPlanners.get_action(boltzmann_policies[n], modified_state)
        push!(actions, act)
    end

    state = PDDL.transition(multi_domain, state, actions)
    push!(state_history, state)

    # for visualizing with ascii
    # print_ascii(state)
    # print("time step: ")
    # print(t)
    # print("\n")

    # if state didn't change, terminate the loop. implies stuck perpetually.
    if state == state_history[t] 
        println("stuck at time step $t")
        break
    end

    # if all the goals have been met, record the time step and terminate the loop
    if all([state[Compound(Symbol("has-filled"), Term[Const(Symbol("agent$n"))])] for n in 1:N])
        println("All goals met at time step $t")
        break
    end
    
end

#print(state_history[end])

#end code splicing
#multi agent RTDP is put on hold.
#planner = MultiAgentRTDP(heuristic=heuristic, rollout_noise=2.0, n_rollouts=50)

# sol = planner(domain, state, mal_spec)
# print(sol.plan)

# simulator = StateActionRecorder(100)
# actions, trajectory = simulator(sol, domain, state, mal_spec)