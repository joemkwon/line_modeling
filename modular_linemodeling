#import libraries
using PDDL, PlanningDomains
using SymbolicPlanners
import SymbolicPlanners: precompute!, is_precomputed, compute
using CSV

#set paths
const domain_path = "/Users/joekwon/Desktop/line_modeling"
const state_history_path = "/Users/joekwon/Desktop/line_modeling/state_histories/"
const data_save_path = "/Users/joekwon/Desktop/line_modeling/maps/data/"
const problem_path = "/Users/joekwon/Desktop/line_modeling/maps"

#variables for simulations
const all_maps = ["no_line_1_test.pddl", "no_line_2_test.pddl", "yes_line_7_test.pddl", "yes_line_B.pddl"]
const T = 500
const N = 8 
const num_iterations = 1
boltzmann_policy_parameters = [0.0001]
#const modeling_type = "oldmodeling"
PDDL.Arrays.@register() 

#heuristic setup
function heuristic_setup()
    # Create a well tank heuristic
    inner_heuristic = WellTankHeuristic() # Optimistic heuristic
    planner = AStarPlanner(inner_heuristic) # Planner that uses optimistic heuristic
    heuristic = PlannerHeuristic(planner) # Some exact heuristic
    memoized_h = memoized(heuristic) # Memoized exact heuristic
    return memoized_h
end

memoized_h = heuristic_setup()

#multi-agent domain setup
struct MultiAgentDomain{D <: Domain} <: Domain
    domain::D
end 

# Define function to create boltzmann policies for each agent
function create_boltzmann_policies(N, domain, mal_spec, noise)
    policies = Array{BoltzmannPolicy}(undef, N)
    for n in 1:N
        agent = Const(Symbol("agent$n"))
        mal_spec = MinStepsGoal(Term[Compound(Symbol("has-filled"), Term[agent])])
        inner_policy = FunctionalVPolicy(memoized_h, domain, mal_spec) # Policy that evaluates every state
        boltzmann_policy = BoltzmannPolicy(inner_policy, noise) # Boltzmann agent
        policies[n] = boltzmann_policy
    end
    return policies
end

# Define function to modify the state
function modify_state(state::State, k::Integer, domain)
    # Get the object types of the state
    objtypes = PDDL.get_objtypes(state)

    agent_locs = Array{Tuple{Int,Int}}(undef, N-1)
    agent_index = 1
    # iterate over all agent objects. Unless it is the current agent, turn it into a wall
    for n in 1:N
        if n == k
            continue
        end
        agent = Const(Symbol("agent$n"))
        #save the coordinates of the agent, in order to place a wall there
        agent_locs[agent_index] = ((state[Compound(:xloc, Term[agent])], state[Compound(:yloc, Term[agent])]))
        agent_index += 1
        #then, delete the agent completely
        delete!(objtypes, agent)
    end

    # Get the old wall matrix and modify it to include walls where agents were
    walls = copy(state[pddl"(walls)"])
    for loc in agent_locs
        walls[loc[2], loc[1]] = true
    end

    # Create a dictionary of fluents and loop over non-agents to assign their locations
    fluents = Dict{Term, Any}()
    for (obj, objtype) in objtypes
        if objtype != :agent
            fluents[Compound(:xloc, Term[obj])] = state[Compound(:xloc, Term[obj])]
            fluents[Compound(:yloc, Term[obj])] = state[Compound(:yloc, Term[obj])]
        end
    end

    # Assign the wall fluent and position of the remaining agent
    agent = Const(Symbol("agent$k"))
    fluents[pddl"(walls)"] = walls
    fluents[Compound(:xloc, Term[agent])] = state[Compound(:xloc, Term[agent])]
    fluents[Compound(:yloc, Term[agent])] = state[Compound(:yloc, Term[agent])]

    # Assign values to boolean fluents
    boolean_fluents = [Symbol("has-filled"), Symbol("has-water1"), Symbol("has-water2"), Symbol("has-water3")]
    for boolean_fluent in boolean_fluents
        fluents[Compound(boolean_fluent, Term[agent])] = state[Compound(boolean_fluent, Term[agent])]
    end

    return initstate(domain, objtypes, fluents)
end

# Define function to manage the state transition
function PDDL.transition(domain::MultiAgentDomain, state::State, actions::Vector{Term})
    agent_locs = []
    
    for (i, act) in enumerate(actions)
        agent = Const(Symbol("agent$i"))
        next_state = transition(domain.domain, state, act)
        next_agent_loc = (next_state[Compound(:xloc, Term[agent])], next_state[Compound(:yloc, Term[agent])])
        
        if next_agent_loc in agent_locs
            agent_loc = (state[Compound(:xloc, Term[agent])], state[Compound(:yloc, Term[agent])]) 
            push!(agent_locs, agent_loc)
        else
            state = next_state
            push!(agent_locs, next_agent_loc)
        end
    end
    
    return state
end

# Define function to get actions for each agent
function get_agent_actions(boltzmann_policies::Vector{BoltzmannPolicy}, state::State, N::Int64, domain)
    actions = Term[]
    for n in 1:N
        agent = Const(Symbol("agent$n"))
        modified_state = modify_state(state, n, domain)
        act = SymbolicPlanners.get_action(boltzmann_policies[n], modified_state)
        push!(actions, act)
    end
    return actions
end

# Define function to write the state history to file
function write_state_history_to_file(file_path::String, state::State, edit_type::String)
    if edit_type == "init"
        file = open(file_path, "w")
    else
        file = open(file_path, "a")
    end
    write(file, string(state))
    close(file)
end

# Define function to load domain and problem for setup
function initialize(domain_path, problem_path, map)
    problem = load_problem(joinpath(problem_path, map))
    domain = load_domain(joinpath(domain_path, "domain.pddl"))
    state, multi_domain, mal_spec = initialize_state(domain, problem)
    return state, domain, multi_domain, mal_spec
end

# Define function to initialize state and get the multi-agent domain
function initialize_state(domain, problem)
    state = initstate(domain, problem)
    multi_domain = MultiAgentDomain(domain)
    mal_spec = Specification(problem)
    return state, multi_domain, mal_spec
end

# Define function to run the simulations
function run_simulations(all_maps, boltzmann_policy_parameters, num_iterations, T,
    domain_path, problem_path, state_history_path, data_save_path)
    for map in all_maps
        
        #data stores the boltzmann noise parameter, the iteration, and the time step of when agents filled
        data = Dict{Float64, Dict{Int64, Dict{Int64, Int64}}}()
        for parameter in boltzmann_policy_parameters
            data[parameter] = Dict{Int64, Dict{Int64, Int64}}()
        end

        #iterate over the various boltzmann noise parameters
        for noise in boltzmann_policy_parameters
            #iterate over the number of iterations we allow for each map
            for iteration in 1:num_iterations
                #initialize the state, domain, multi_domain, and mal_spec
                state, domain, multi_domain, mal_spec = initialize(domain_path, problem_path, map)
                state_history = [state]

                #get the boltzmann policies for each agent
                boltzmann_policies = create_boltzmann_policies(N, domain, mal_spec, noise)

                #initialize the agent filled dictionary, which stores the time step at which each agent filled
                agent_filled = Dict{Int64, Int64}(1 => 0, 2 => 0, 3 => 0, 4 => 0, 5 => 0, 6 => 0, 7 => 0, 8 => 0)

                #write the initial state to file
                state_history_name = "state_history_$(map)_$(noise)_iteration$(iteration).txt"
                write_state_history_to_file(joinpath(state_history_path, state_history_name), state, "init")

                #iterate over the time steps
                for t in 1:T
                    #get the actions for each agent, and transition the state
                    actions = get_agent_actions(boltzmann_policies, state, N, domain)
                    state = PDDL.transition(multi_domain, state, actions)

                    #write the state to file
                    write_state_history_to_file(joinpath(state_history_path, state_history_name), state, "append")
                    push!(state_history, state)
            
                    #update the agent filled dictionary if the agent has filled
                    for n in 1:N
                        agent = Const(Symbol("agent$n"))
                        #only change the value if it is equal to 0, otherwise constant overwriting.
                        if state[Compound(Symbol("has-filled"), Term[agent])] && agent_filled[n] == 0
                            agent_filled[n] = t
                        end
                    end
            
                    #check if the state has repeated itself, if so, break. This is to handle the case where the agents get stuck in a loop
                    if state == state_history[t] && state == state_history[t-1] && state == state_history[t-2] 
                        for n in 1:N
                            agent_filled[n] = -1
                        end
                        break
                    end
                    
                    #check if all agents have filled, if so, break
                    if all([state[Compound(Symbol("has-filled"), Term[Const(Symbol("agent$n"))])] for n in 1:N])
                        break
                    end
                end

                #write the data to our dictionary
                data[noise][iteration] = agent_filled
            end
            #save the data to file
            csv_name = "$(data_save_path)$(map)_$(noise).csv"
            CSV.write(csv_name, data, append=true)
        end
    end
    print("Finished!")
end

run_simulations(all_maps, boltzmann_policy_parameters, num_iterations, T,
    domain_path, problem_path, state_history_path, data_save_path)

#TODO: add in what kind of modeling we want into the run_simulations function


